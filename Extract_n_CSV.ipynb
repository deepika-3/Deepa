{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcdhjoIvZKQrRd6UxJopKT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepika-3/Deepa/blob/main/Extract_n_CSV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqdk2atW5U-N",
        "outputId": "47d9ffe4-6cae-46fa-9f30-e0b11df41357"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been successfully saved to extracted_data.csv.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import quote\n",
        "import csv\n",
        "\n",
        "def extract_text_from_url(url):\n",
        "    try:\n",
        "        # Properly format the URL\n",
        "        formatted_url = quote(url, safe=':/')\n",
        "\n",
        "        # Specify headers to send with the request\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Accept-Encoding': 'gzip, deflate, br',\n",
        "            'Connection': 'keep-alive',\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'\n",
        "        }\n",
        "\n",
        "        # Send a GET request to the URL with headers\n",
        "        response = requests.get(formatted_url, headers=headers)\n",
        "\n",
        "        # Check if the request was successful (status code 200)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Parse the HTML content of the page using BeautifulSoup\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract text from the parsed HTML\n",
        "        text = soup.get_text()\n",
        "\n",
        "        return text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to retrieve content from {url}. Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example usage:\n",
        "urls = [\"https://insights.blackcoffer.com\"]\n",
        "extracted_texts = {}\n",
        "\n",
        "for url in urls:\n",
        "    text = extract_text_from_url(url)\n",
        "    extracted_texts[url] = text\n",
        "\n",
        "# Store data in a CSV file\n",
        "csv_file_path = 'extracted_data.csv'\n",
        "\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write header\n",
        "    csv_writer.writerow(['URL', 'Text'])\n",
        "\n",
        "    # Write data\n",
        "    for url, text in extracted_texts.items():\n",
        "        csv_writer.writerow([url, text])\n",
        "\n",
        "print(f'Data has been successfully saved to {csv_file_path}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "FN1FzTAg56pK"
      }
    }
  ]
}